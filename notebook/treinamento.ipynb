{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zz4VkUe5SGks"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate peft bitsandbytes torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import get_peft_model, LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "L3gGzf-8SZF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nome do modelo\n",
        "MODEL_NAME = \"tiiuae/falcon-7b\""
      ],
      "metadata": {
        "id": "vXfj9_soSpPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qnB78q6qStP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "pe1SXyvgY_Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Carregar o modelo com quantiza√ß√£o para economizar mem√≥ria\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,  # Usa menos VRAM\n",
        "    device_map=\"auto\"  # Distribui automaticamente entre CPU e GPU\n",
        ")"
      ],
      "metadata": {
        "id": "xXzkXRl-Sw1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {\"input\": \"O que √© intelig√™ncia artificial?\", \"output\": \"√â a capacidade das m√°quinas de realizar tarefas que normalmente exigiriam intelig√™ncia humana.\"},\n",
        "    {\"input\": \"Quem foi Alan Turing?\", \"output\": \"Alan Turing foi um matem√°tico e cientista da computa√ß√£o brit√¢nico, considerado um dos pais da computa√ß√£o moderna.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "n83A_dzjS0EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter para Dataset Hugging Face\n",
        "dataset = Dataset.from_list(data)"
      ],
      "metadata": {
        "id": "Bc3dCiLlS9C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    prompt = \"Pergunta: \" + examples[\"input\"] + \"\\nResposta: \" + examples[\"output\"]\n",
        "\n",
        "    # Tokenizar entrada e sa√≠da juntas\n",
        "    tokens = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Criar labels: r√≥tulos s√£o os mesmos input_ids, mas ignoramos o padding (-100)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    tokens[\"labels\"] = [\n",
        "        -100 if token == tokenizer.pad_token_id else token for token in tokens[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "JYt4TpEbS-O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar o dataset\n",
        "dataset = dataset.map(tokenize_function)"
      ],
      "metadata": {
        "id": "_rBgTj1YTDGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "rZdjL8KVVD2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Define o tamanho das matrizes auxiliares LoRA\n",
        "    lora_alpha=32,  # Define a escala do ajuste LoRA\n",
        "    lora_dropout=0.05,  # Adiciona dropout para evitar overfitting\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM  # Define o modelo como um \"causal language model\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ys3_R6ONTHMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar LoRA ao Falcon 7B\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "fc28x0WOTN3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir os par√¢metros trein√°veis\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "BTFv5h9HTOxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar os hiperpar√¢metros do treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./falcon-7b-lora-finetuned\",  # Onde salvar o modelo treinado\n",
        "    per_device_train_batch_size=2,  # Usa batch pequeno para economizar VRAM\n",
        "    gradient_accumulation_steps=4,  # Simula batch maior sem estourar a VRAM\n",
        "    num_train_epochs=3,  # N√∫mero de √©pocas de treinamento\n",
        "    learning_rate=2e-5,  # Taxa de aprendizado otimizada para LoRA (0.00002)\n",
        "    logging_dir=\"./logs\",  # Diret√≥rio de logs para an√°lise\n",
        "    logging_steps=10,  # Salvar logs a cada 10 steps\n",
        "    save_strategy=\"epoch\",  # Salvar checkpoints no final de cada √©poca\n",
        "    fp16=True,  # Usa FP16 para reduzir o consumo de VRAM\n",
        "    push_to_hub=False,  # Se quiser salvar no Hugging Face, mude para True\n",
        "    report_to=\"none\"  # üöÄ Isso desativa o W&B corretamente!\n",
        ")"
      ],
      "metadata": {
        "id": "_cCmoJu9TiGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Iniciar o treinamento\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "v5GvbXBXTsJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar modelo treinado\n",
        "model.save_pretrained(\"./falcon-7b-lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"./falcon-7b-lora-finetuned\")"
      ],
      "metadata": {
        "id": "VIeIL2-BTydM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testar gera√ß√£o de texto\n",
        "input_text = \"Quem descobriu o Brasil?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")"
      ],
      "metadata": {
        "id": "yEiK2JUkT-pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar resposta com o modelo treinado\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
        "    max_length=500,\n",
        "    temperature=1.0,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,  # üöÄ Penaliza palavras repetidas\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nüîπ Resposta Gerada:\\n\", generated_text)"
      ],
      "metadata": {
        "id": "-iXD0vAFUJzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}