# Fine-Tuning do Falcon 7B com LoRA

Este reposit√≥rio contem um notebook completo para realizar fine-tuning do modelo Falcon 7B utilizando LoRA (Low-Rank Adaptation). O objetivo √© permitir a personaliza√ß√£o desse LLM (Large Language Model) sem precisar modificar todos os seus pesos, tornando o processo mais eficiente e acess√≠vel.

## Contextualiza√ß√£o

Fine-Tuning √© uma t√©cnica utilizada para ajustar um modelo de IA pr√©-treinado em um novo conjunto de dados, permitindo que ele aprenda novas informa√ß√µes sem perder o conhecimento pr√©vio.

**Por que isso √© importante?**
Os modelos de linguagem grandes (LLMs) s√£o treinados em bilh√µes de textos, mas nem sempre conhecem informa√ß√µes espec√≠ficas. Com fine-tuning, podemos adaptar um modelo a dom√≠nios espec√≠ficos, como atendimento ao cliente, medicina, direito ou engenharia.

**Este projeto permite:**

- Ajustar o Falcon 7B para responder perguntas personalizadas.
- Utilizar LoRA para reduzir o consumo de mem√≥ria.
- Criar uma API para servir o modelo ap√≥s o treinamento.

## O que √© o Falcon 7B?

O Falcon 7B √© um LLM (Large Language Model) desenvolvido pelo Technology Innovation Institute (TII). Ele faz parte da fam√≠lia de modelos Falcon e foi treinado em textos de alta qualidade para gerar respostas precisas e coerentes.

**Caracter√≠sticas do Falcon 7B:**

- Possui 7 bilh√µes de par√¢metros.
- Modelo causal (Causal Language Model - CLM), usado para prever a pr√≥xima palavra.
- Open-source, permitindo personaliza√ß√µes.
- Suporta fine-tuning com LoRA, tornando o treinamento mais eficiente.

**Por que escolhi o Falcon 7B?**

- Ele oferece um √≥timo equil√≠brio entre desempenho e custo computacional.
- Pode ser treinado em GPUs acess√≠veis (como A100 e RTX 3090).
- Open-source, permitindo modifica√ß√µes e personaliza√ß√µes.

## Quando fazer fine-tuning?

Nem sempre precisamos treinar um LLM do zero. O fine-tuning √© √∫til quando queremos especializar o modelo em um dom√≠nio ou tarefa espec√≠fica.

**Quando fazer Fine-Tuning?**

- Quando o modelo base n√£o entende um dom√≠nio espec√≠fico (Ex: medicina, direito, engenharia).
- Quando precisamos de um chatbot personalizado (Ex: um assistente para suporte t√©cnico).
- Quando queremos que o modelo siga um tom de linguagem espec√≠fico (Ex: respostas mais formais ou descontra√≠das).
- Quando queremos adicionar novos conhecimentos sem treinar um modelo do zero (Ex: modelos de IA jur√≠dicos).

**Vantagens do Fine-Tuning:**

- Evita ter que treinar um modelo do zero, economizando tempo e custo computacional.
- Melhora a precis√£o em tarefas espec√≠ficas, como atendimento ao cliente ou gera√ß√£o de c√≥digo.
- Funciona mesmo em GPUs menores usando t√©cnicas como LoRA.

üí° Se o modelo base j√° responde bem √†s suas necessidades, voc√™ pode apenas us√°-lo diretamente sem fazer fine-tuning.

## Estrutura do Reposit√≥rio

| **Diret√≥rio/Arquivo** | **Descri√ß√£o**                               |
| --------------------- | ------------------------------------------- |
| üìú `README.md`        | Documenta√ß√£o do projeto                     |
| üìÇ `notebooks/`       | Cont√©m o notebook de treinamento            |
| üìÇ `models/`          | Diret√≥rio onde o modelo treinado ser√° salvo |
| üìÇ `api/`             | C√≥digo da API para servir o modelo          |

## Como usar este projeto?

### Por que voc√™ precisa de uma GPU para o treinamento?

Os modelos de linguagem grandes (LLMs), como o Falcon 7B, possuem bilh√µes de par√¢metros e exigem m√∫ltiplos c√°lculos de matriz durante o treinamento. Isso torna imprescind√≠vel o uso de uma GPU (Unidade de Processamento Gr√°fico), que pode acelerar os c√°lculos em compara√ß√£o com uma CPU.

**Benef√≠cios do uso de GPU no treinamento:**

- Processamento muito mais r√°pido do que CPUs para c√°lculos matriciais.
- Melhor aproveitamento da mem√≥ria VRAM, permitindo treinar grandes modelos.
- Compatibilidade com bibliotecas otimizadas como PyTorch + CUDA.

**Usando o Google Colab Pro para o treinamento**
Se voc√™ n√£o tem uma GPU local potente, o Google Colab Pro √© uma √≥tima op√ß√£o para treinar modelos de IA. Ele oferece acesso a GPUs poderosas, como NVIDIA A100 e V100, por um custo acess√≠vel.

**Vantagens do Google Colab Pro:**

- Permite treinar modelos sem precisar de uma GPU local.
- Acesso a GPUs NVIDIA A100 (40GB VRAM) para processar modelos grandes.
- Integra√ß√£o f√°cil com Google Drive para salvar checkpoints do modelo.

üìå Este notebook foi desenvolvido e testado no Google Colab Pro, utilizando uma GPU NVIDIA A100.

### Treinamento do modelo

O notebook inclui os passos para fazer fine-tuning do Falcon 7B com LoRA.
Basta executar as c√©lulas no Jupyter Notebook ou no Google Colab Pro.

üìå Dica: No Google Colab, ative o uso de GPU A100 acessando:

`Runtime > Change runtime type > Hardware accelerator > GPU.`

#### Realizando infer√™ncias

Ap√≥s o treinamento, voc√™ pode disponibilizar o modelo por meio de uma API REST que est√° no diret√≥rio `api` que foi constru√≠da utilizando FastAPI. Para isso, fa√ßa o download do modelo, que voc√™ acobou de treinar no Colab e salve-o no diret√≥rio `model` desse reposit√≥rio.

#### Instale as depend√™ncias:

```bash
pip install transformers datasets accelerate peft bitsandbytes fastapi uvicorn

```

#### Execute o seguinte comando para iniciar o servidor:

```bash
uvicorn api:app --host 0.0.0.0 --port 8000
```

Agora, sua API estar√° dispon√≠vel e voc√™ pode visualizar o Swagger no endere√ßo abaixo:

```
http://localhost:8000/docs
```

#### Teste via `curl` no Terminal

Teste a API diretamente pelo terminal:

```bash
curl -X 'POST' \
  'http://localhost:8000/generate/' \
  -H 'Content-Type: application/json' \
  -d '{"prompt": "O futuro da intelig√™ncia artificial √©", "max_length": 50, "temperature": 0.7}'
```

## Contribui√ß√£o e Melhorias

- Se quiser contribuir com melhorias, fique √† vontade para enviar um Pull Request ou abrir uma Issue.
- Se tiver d√∫vidas, entre em contato!
